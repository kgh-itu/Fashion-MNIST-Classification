\section{Decision Tree}
In this section, we will discuss how the \class{DecisionTreeClassifier()} model is implemented and how the hyperparameters are chosen, as well as the results of using this model.\\

A decision tree is a supervised machine learning model that can be used for classification or regression tasks.
It works by building a tree-like structure, where at each internal node, the algorithm evaluates all the available features and selects the split that maximizes the purity of the resulting splits.
This process is repeated until a stopping condition is reached, such as a maximum depth or no further improvement in purity.
The resulting leaf nodes are also known as decision regions, and the prediction for a sample is made by traversing the tree to find the correct decision region and then predicting the most common class in that region.
Decision trees are popular because they are easy to understand and interpret due to their tree-like structure, and they can handle different types of data and decision boundaries.
However, during training, the algorithm only considers the best split for each internal node without considering the potential negative effects on future splits, which is known as a greedy approach.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{figures_for_report/example_decision_tree}
    \captionsetup{justification=centering,margin=2cm}
    \caption{Simple Decision Tree}
\end{figure}


\subsection{Implementation}
As this project is focused on classification rather than regression a classification tree was implemented.
It was done in Python using two classes:\\
\begin{enumerate}
    \item \class{Node()}
    \item \class{DecisionTreeClassifier()}
    \end{enumerate}
\vspace{10pt}

\subsubsection{Node()}
The \class{Node()} class has the most responsibility of the two.
It holds the data and passes it down the tree constantly splitting itself into more nodes with the \code{_split()} method.
This is done by finding the best split with the \code{_get_best_split()} method based on the impurity measure specified.
The best split refers to the feature and cutoff value that leads to the highest gain in purity in the two resulting nodes.
After finding the best split, it can then create two new nodes, \code{self.left} and \code{self.right} if all split criteria are fulfilled.
The two criteria implemented are \code{max_depth} and \code{min_samples_split} and are discussed further in the \class{DecisionTreeClassifier()} implementation.

\subsubsection{DecisionTreeClassifier()}
The \class{DecisionTreeClassifier()} is the classifier class.
which has the \code{fit()} and \code{predict()} methods and some 'less' important
methods \code{get_depth()} and \code{get_n_leaves()}, that can be called after the model has been fitted.
The \code{fit(X, y)} method takes the following two parameters, a feature matrix \code{(X)} and class labels \code{(y)}
This data fed into the first node in the tree (the root).
The purity of the node is then found and if a split can improve the purity and all split criteria are fulfilled the node calls \code{_split()} on itself.
This happens recursively and is how the tree-structure is created.
The \code{predict(X)} method takes an array of samples or a single sample, and predicts the corresponding label.
It works by traversing down the tree until a leaf node is reached, and then predicting the most frequent class in that node.
The constructor take the following 4 parameters.\\
\begin{itemize}
    \item \code{max_depth} - (An integer - the maximum depth to which the tree can grow)
    \item \code{min_samples_split} - (An integer - minimum number of samples in a node before for it to split)
    \item \code{criterion} - (A string - impurity measure (gini or entropy) to use when splitting)
    \item \code{random_state} - (An integer - used to set the random seed for reproducible results)
    \end{itemize}
\vspace{10pt}

\subsection{Correctness}
To validate the correctness of our implementation a thorough comparison with the sklearn version was done.
First a comparison of 100 sklearn models and 100 of our models accuracy score was done using the \code{load_digits()} dataset from \code{sklearn.datasets}
This can be seen in Figure 5.\\

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.55]{figures_for_report/our_vs_sklearn_accuracy}
    \captionsetup{justification=centering,margin=2cm}
    \caption{Accuracy distribution from 100 models}
\end{figure}

Each decision tree used in Figure 5 is fitted with the default parameters of both the sklearn and our model.
This means that the \code{max_depth} is not restricted and the \code{min_samples_split} is set to 2.
A decision tree has randomness involved in the training process.
This is because if two splits yield the same gain one of them will be chosen at random.
This means the models might perform slightly different in a single test but very similar on average.\\

The resulting tree depths and number of leaves were also compared and showed that the decision trees in both implementations grow to the same
depth and with the same number of leaf nodes.
\\

Lastly a comparison of \textbf{some} of the actual splits (best feature and cutoff) were done to assert correctness and as a sanity check, and
the two implementations also agreed on this.\\

\subsection{Hyperparameters}
A decision tree is very prone to overfitting to the training data.
This is especially the case if the max depth or other early stopping to the growth is not controlled.
This can lead to a  very high training accuracy but might struggle with unseen data.
Another way to control this is using post-pruning where the tree is first grown fully, and then afterwards its size is reduced.
This was not implemented in our model so what was tried instead was to find the optimal \code{max_depth}, to reduce overfitting.
Along with finding an optimal \code{max_depth}, a good combination of \code{max_depth}, \code{min_samples_split} and
\code{criterion} (gini or entropy) was found.
This was done \class{GridSearchCV()} from sklearn, where a range of values for each of the mentioned parameters was searched for.
\class{GridSearchCV()} uses cross-validation, in this case 5-fold, on the training data, and tests all combinations specified in the following parameter grid:

\begin{center} \begin{minipage}{4in}

\begin{itemize}
    \item \code{max_depth}: [None, 5, 7, 9, 11, 12]
    \item \code{min_samples_split}: [2, 4, 8, 10]
    \item \code{criterion}: [gini, entropy] \\
\end{itemize}
            \end{minipage}
\end{center}

This resulted in the following best combination of hyperparameters \code{max_depth=9}, \code{min_samples_split=4}, \code{criterion=gini}.

\subsection{Results}


\begin{table}[!ht]
\begin{subtable}[c]{0.4\textwidth}
\footnotesize
\centering
\begin{tabular}{ c | c }
 \toprule
 Evaluation Metric & Accuracy Score  \\
 \midrule
 Training Accuracy &  89.83\% \\
 Test Accuracy &79.32\% \\
 \bottomrule
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\end{subtable}
\begin{subtable}[c]{0.6\textwidth}
\footnotesize
\centering
\begin{tabular}{c | c c r}
Class & Precision & Recall & F1-Score\\
\midrule
T-shirt/Top   &    0.76  &    0.75  &    0.75 \\
Trousers   &    0.97  &    0.92  &    0.95 \\
Pullover   &    0.80  &    0.83  &    0.81\\
Dress   &    0.82  &    0.88  &    0.85\\
Shirt   &    0.61  &    0.60  &    0.60\\
\end{tabular}
\captionsetup{justification=centering,margin=1cm}
\end{subtable}
\caption{Decision Tree Performance Evaluation}
\label{dt_evaluation}
\end{table}\\

The results reported in table 2 are results of our implementation of the \class{DecisionTreeClassifier()} with the parameters specified in section \textbf{4.3}.
To reproduce these exact results it should additionally be initialized with \code{random_state=42}, to eliminate the randomness.



\subsubsection{Discussing Results}







